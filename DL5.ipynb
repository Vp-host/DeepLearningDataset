{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Student Details**\n",
    "Name: Vishal Pattar  \n",
    "Roll no: 43556  \n",
    "Class: BE AIML  \n",
    "Subject: Deep Learning for AI  \n",
    "Assignment: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Problem Statement:**\n",
    "\n",
    "Implement the Continuous Bag of Words (CBOW) Model for word embedding. The implementation should include the following stages:\n",
    "- Data preparation\n",
    "- Generate training data\n",
    "- Train the model\n",
    "- Output word embeddings\n",
    "\n",
    "Use a suitable text dataset such as the Brown Corpus or any other sizable text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Lambda, Dot, Reshape, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample dataset: For demonstration, using a small text. Replace with a larger corpus as needed.\n",
    "sample_text = \"\"\"\n",
    "In the age of information, data is the new oil. The ability to process and analyze data\n",
    "has become a crucial skill in various industries. Machine learning and artificial intelligence\n",
    "are driving innovations that were once thought impossible.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sample_text])\n",
    "word2id = tokenizer.word_index\n",
    "id2word = {v: k for k, v in word2id.items()}\n",
    "vocab_size = len(word2id) + 1  # +1 for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define window size\n",
    "window_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate skip-grams pairs\n",
    "sequences = tokenizer.texts_to_sequences([sample_text])[0]\n",
    "pairs, labels = skipgrams(sequences, vocabulary_size=vocab_size, window_size=window_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate target and context words\n",
    "target_words, context_words = zip(*pairs)\n",
    "target_words = np.array(target_words, dtype='int32')\n",
    "context_words = np.array(context_words, dtype='int32')\n",
    "labels = np.array(labels, dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stage c: Train the model\n",
    "embedding_dim = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input for target words\n",
    "target_input = Input(shape=(1,), name='target_input')\n",
    "# Input for context words\n",
    "context_input = Input(shape=(1,), name='context_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding layer\n",
    "embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name='embedding_layer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed target and context words\n",
    "target_embedding = embedding(target_input)\n",
    "context_embedding = embedding(context_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute dot product between target and context embeddings\n",
    "dot_product = Dot(axes=-1)([target_embedding, context_embedding])\n",
    "dot_product = Reshape((1,))(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output\n",
    "output = Dense(1, activation='sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    }
   ],
   "source": [
    "# Define and compile the model\n",
    "cbow_model = Model(inputs=[target_input, context_input], outputs=output)\n",
    "cbow_model.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.001), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "5/5 - 1s - loss: 0.6939 - accuracy: 0.4863 - 700ms/epoch - 140ms/step\n",
      "Epoch 2/10\n",
      "5/5 - 0s - loss: 0.6929 - accuracy: 0.6199 - 9ms/epoch - 2ms/step\n",
      "Epoch 3/10\n",
      "5/5 - 0s - loss: 0.6919 - accuracy: 0.7158 - 12ms/epoch - 2ms/step\n",
      "Epoch 4/10\n",
      "5/5 - 0s - loss: 0.6910 - accuracy: 0.7877 - 11ms/epoch - 2ms/step\n",
      "Epoch 5/10\n",
      "5/5 - 0s - loss: 0.6900 - accuracy: 0.8082 - 17ms/epoch - 3ms/step\n",
      "Epoch 6/10\n",
      "5/5 - 0s - loss: 0.6889 - accuracy: 0.8322 - 16ms/epoch - 3ms/step\n",
      "Epoch 7/10\n",
      "5/5 - 0s - loss: 0.6877 - accuracy: 0.8356 - 16ms/epoch - 3ms/step\n",
      "Epoch 8/10\n",
      "5/5 - 0s - loss: 0.6863 - accuracy: 0.8390 - 17ms/epoch - 3ms/step\n",
      "Epoch 9/10\n",
      "5/5 - 0s - loss: 0.6847 - accuracy: 0.8459 - 14ms/epoch - 3ms/step\n",
      "Epoch 10/10\n",
      "5/5 - 0s - loss: 0.6829 - accuracy: 0.8425 - 12ms/epoch - 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x20b84fbd590>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "cbow_model.fit([target_words, context_words], labels, epochs=10, batch_size=64, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the embeddings from the trained model\n",
    "word_embeddings = cbow_model.get_layer('embedding_layer').get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the embedding of a word\n",
    "def get_embedding(word):\n",
    "    if word in word2id:\n",
    "        return word_embeddings[word2id[word]]\n",
    "    else:\n",
    "        return np.zeros(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding for \"data\":\n",
      "[-0.01710158  0.0116161  -0.02464188 -0.03927892  0.05300517 -0.00230624\n",
      " -0.03641287 -0.04884042 -0.0496954   0.04452392  0.02144654  0.00954155\n",
      "  0.01109463 -0.055489    0.01203961  0.05245222  0.05445194  0.04193056\n",
      "  0.00500811  0.03556725 -0.03247273 -0.01749837 -0.0309183  -0.00319361\n",
      "  0.0517065   0.01266043 -0.01370715  0.04745135  0.02651867 -0.00543878\n",
      " -0.00648857  0.03327759 -0.00078968  0.01683643 -0.0657592   0.02087454\n",
      " -0.01425486 -0.03813679  0.03883107 -0.01430559  0.04702204  0.05680364\n",
      "  0.02391579  0.00052433 -0.02342741 -0.00980793 -0.02570198 -0.01734126\n",
      "  0.00143671  0.01423475]\n",
      "\n",
      "Embedding for \"machine\":\n",
      "[ 0.07005596  0.04625028  0.02980367 -0.02960505 -0.04380663 -0.01746259\n",
      " -0.03321838  0.02475217 -0.02357429  0.03886066  0.07400706 -0.07872978\n",
      "  0.02373296 -0.01008565  0.07244141 -0.03176982 -0.03403271  0.01258605\n",
      "  0.01888227 -0.00208324  0.01559594 -0.04067697  0.0543967  -0.01956667\n",
      " -0.02364531 -0.06183666  0.05409291  0.00216286 -0.02790195 -0.06084784\n",
      " -0.05789266  0.0640365   0.03976301  0.00962962  0.01955849 -0.01255633\n",
      "  0.03730505 -0.0191968   0.07206675 -0.04451222 -0.03346168  0.01650729\n",
      " -0.03411786 -0.03650782 -0.04171353  0.02917353 -0.05019526 -0.04986177\n",
      "  0.0151734   0.00070408]\n",
      "\n",
      "Embedding for \"learning\":\n",
      "[ 0.01026295  0.0134474   0.05952498  0.0206053  -0.00578822  0.04525546\n",
      " -0.00373323  0.0020731   0.00958702  0.00183012  0.03367848 -0.03671042\n",
      " -0.02758745  0.02204693  0.02854457 -0.01426513 -0.03192746  0.03940642\n",
      " -0.03730409  0.01418637  0.04431076 -0.0333616   0.01546642  0.03784681\n",
      " -0.03515205 -0.03335139  0.04971796 -0.03499226 -0.03792679 -0.02313475\n",
      " -0.03744177  0.03054076  0.02334055 -0.01615841  0.020174   -0.0146824\n",
      "  0.01630571 -0.00014461  0.04313596 -0.00227033 -0.03680923 -0.03320541\n",
      " -0.00284553  0.02137488 -0.01067232  0.01124227 -0.012409   -0.0208065\n",
      "  0.0315834   0.03351729]\n",
      "\n",
      "Embedding for \"information\":\n",
      "[ 0.01057294  0.04857907  0.01302409 -0.00210707 -0.01894209 -0.02752216\n",
      " -0.05721286 -0.00290365 -0.04354713  0.04099433  0.04782293 -0.04932134\n",
      " -0.01324701 -0.04807686  0.05832585  0.05397854  0.03265424  0.05266983\n",
      "  0.00274226 -0.00747234  0.05779222  0.01536057 -0.05206069  0.0101895\n",
      "  0.07785959  0.08526028  0.04687314  0.03043287  0.07667422  0.00408414\n",
      "  0.01066621  0.02902564 -0.01076889 -0.00399081 -0.09250367  0.00705038\n",
      " -0.0489012  -0.07551999  0.03124234 -0.07085251  0.00150133  0.06792476\n",
      "  0.01939665 -0.06849415 -0.03790807  0.04000076 -0.06959794 -0.06588417\n",
      "  0.03233319 -0.00682864]\n",
      "\n",
      "Embedding for \"innovation\":\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example: Get embeddings for some words\n",
    "words = ['data', 'machine', 'learning', 'information', 'innovation']\n",
    "for word in words:\n",
    "    emb = get_embedding(word)\n",
    "    print(f'Embedding for \"{word}\":\\n{emb}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
